# -*- coding: utf-8 -*-
"""hw6_works.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pKjLYnPfUU8tOMNZSQDDDxoXeSYRB0_r
"""

from google.colab import drive
drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/My Drive/ece_695/DLStudio-1.1.2

# Commented out IPython magic to ensure Python compatibility.
# %cd DLStudio/

pip install pymsgbox

import gzip
import pickle
from DLStudio import *

# Commented out IPython magic to ensure Python compatibility.
# %cd ..

#Task 1
class ModifiedSentimentAnalysisDataset(DLStudio.TextClassification.SentimentAnalysisDataset, torch.utils.data.Dataset):
            def __init__(self, dl_studio, train_or_test, dataset_file):
                super().__init__(dl_studio, train_or_test, dataset_file)
               

            def one_hotvec_for_word(self, word):
                word_index =  self.vocab.index(word)
                hotv=torch.tensor(word_index)
                return hotv

            def review_to_tensor(self, review):                
                review_tensor = torch.zeros(len(review),1)                
                for i,w in enumerate(review):
                    review_tensor[i,:] = self.one_hotvec_for_word(w)
                return review_tensor

def run_code_for_training_for_text_classification_with_gru(net, hidden_size, text_cl):        
             #filename_for_out = "performance_numbers_" + str(dls.epochs) + ".txt"
            #FILE = open(filename_for_out, 'w')
            net = copy.deepcopy(net)
            net = net.to(dls.device)
            criterion = nn.NLLLoss()
#            criterion = nn.MSELoss()
#            criterion = nn.CrossEntropyLoss()
            accum_times = []
            optimizer = optim.SGD(net.parameters(), 
                         lr=dls.learning_rate, momentum=dls.momentum)
            for epoch in range(dls.epochs):  
                print("")
                running_loss = 0.0
                start_time = time.clock()
                for i, data in enumerate(text_cl.train_dataloader):    
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    review_tensor = review_tensor.to(dls.device)
                    sentiment = sentiment.to(dls.device)
                    optimizer.zero_grad()
                    hidden = net.init_hidden(review_tensor.shape[0]).to(dls.device)
                    #for k in range(review_tensor.shape[1]):
                    output, hidden = net(review_tensor, hidden)
                    loss = criterion(output, torch.argmax(sentiment, 1))
                    running_loss += loss.item()
                    loss.backward()
                    optimizer.step()
                    if i % 100 == 99:    
                        avg_loss = running_loss / float(100)
                        current_time = time.clock()
                        time_elapsed = current_time-start_time
                        print("[epoch:%d  iter:%4d]     loss: %.3f" % (epoch+1,i+1,avg_loss))
                        accum_times.append(current_time-start_time)
                        running_loss = 0.0



def run_code_for_testing_text_classification_with_gru(net, hidden_size, text_cl):
            #net.load_state_dict(torch.load(dls.path_saved_model))
            classification_accuracy = 0.0
            negative_total = 0
            positive_total = 0
            confusion_matrix = torch.zeros(2,2)
            net = net.to(dls.device)
            with torch.no_grad():
                for i, data in enumerate(text_cl.test_dataloader):
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    review_tensor = review_tensor.to(dls.device)
                    sentiment = sentiment.to(dls.device)
                    #print(review_tensor.shape)
                    #return 0
                    hidden = net.init_hidden(review_tensor.shape[0]).to(dls.device)
                    #for k in range(review_tensor.shape[1]):
                    output, hidden = net(review_tensor, hidden)
                    for j in range(len(output)):
                      predicted_idx = torch.argmax(output[j]).item()
                      gt_idx = torch.argmax(sentiment[j]).item()
                      #if i % 100 == 99:
                      #    print("   [i=%d]    predicted_label=%d       gt_label=%d\n\n" % (i+1, predicted_idx,gt_idx))
                      if predicted_idx == gt_idx:
                          classification_accuracy += 1
                      if gt_idx is 0: 
                          negative_total += 1
                      elif gt_idx is 1:
                          positive_total += 1
                      confusion_matrix[gt_idx,predicted_idx] += 1
            out_percent = np.zeros((2,2), dtype='float')
            #print("\n\nNumber of positive reviews tested: %d" % positive_total)
            #print("\n\nNumber of negative reviews tested: %d" % negative_total)
            print("\nconfusion matrix result:\n")
            out_str = "                      "
            out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
            print(out_str + "\n")
            for i,label in enumerate(['true negative', 'true positive']):
                out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
                out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
                out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
                out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
                out_str = "%12s:  " % label
                for j in range(2):
                    out_str +=  "%18s" % out_percent[i,j]
                print(out_str)

#!/usr/bin/env python

##  text_classification_with_gru.py

"""
This script is an attempt at solving the sentiment classification problem
with an RNN that uses a GRU to get around the problem of vanishing gradients
that are common to neural networks with feedback.
"""

import random
import numpy
import torch
import os, sys


seed = 0           
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
numpy.random.seed(seed)
torch.backends.cudnn.deterministic=True
torch.backends.cudnn.benchmarks=False
os.environ['PYTHONHASHSEED'] = str(seed)


dls = DLStudio(
#                  dataroot = "/home/kak/TextDatasets/",
                  dataroot = "./Examples/data/",
                  path_saved_model = "./saved_model",
                  momentum = 0.5,
                  learning_rate =  1e-4,
                  epochs = 1,
                  batch_size = 1,
                  classes = ('negative','positive'),
                  debug_train = 1,
                  debug_test = 1,
                  use_gpu = True,
              )

text_cl = DLStudio.TextClassification( dl_studio = dls )
dataserver_train = ModifiedSentimentAnalysisDataset(
                                 train_or_test = 'train',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_train_3.tar.gz",
#                                dataset_file = "sentiment_dataset_train_200.tar.gz", 
                                 dataset_file = "sentiment_dataset_train_40.tar.gz", 
                                                                      )
dataserver_test = ModifiedSentimentAnalysisDataset(
                                 train_or_test = 'test',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_test_3.tar.gz",
#                                dataset_file = "sentiment_dataset_test_200.tar.gz",
                                 dataset_file = "sentiment_dataset_test_40.tar.gz",
                                                                  )
text_cl.dataserver_train = dataserver_train
text_cl.dataserver_test = dataserver_test

text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)

vocab_size = 1
hidden_size = 512
output_size = 2                            # for positive and negative sentiments
n_layers = 2

model = text_cl.GRUnet(vocab_size, hidden_size, output_size, n_layers)

number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

num_layers = len(list(model.parameters()))

print("\n\nTask 1")
run_code_for_training_for_text_classification_with_gru(model, hidden_size,text_cl)
run_code_for_testing_text_classification_with_gru(model, hidden_size,text_cl)

#Task 2
class ModifiedTEXTnetOrder2(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dls):
        super(ModifiedTEXTnetOrder2, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.combined_to_hidden = nn.Linear(input_size + 2*hidden_size, hidden_size)
        self.combined_to_middle = nn.Linear(input_size + 2*hidden_size, 100)
        self.middle_to_out = nn.Linear(100, output_size)     
        self.logsoftmax = nn.LogSoftmax(dim=1)
        self.dropout = nn.Dropout(p=0.1)
        # for the cell
        self.cell = torch.zeros(1, hidden_size).to(dls.device)
        self.linear_for_cell = nn.Linear(hidden_size, hidden_size)

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden, self.cell), 1)
        hidden = self.combined_to_hidden(combined)
        out = self.combined_to_middle(combined)
        out = torch.nn.functional.relu(out)
        out = self.dropout(out)
        out = self.middle_to_out(out)
        out = self.logsoftmax(out)
        hidden_clone = hidden.clone()
        self.cell = torch.sigmoid(self.linear_for_cell(hidden_clone)).detach()
        return out,hidden

torch.autograd.set_detect_anomaly(True)

import random
import numpy
import torch
import os, sys


seed = 0           
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
numpy.random.seed(seed)
torch.backends.cudnn.deterministic=True
torch.backends.cudnn.benchmarks=False
os.environ['PYTHONHASHSEED'] = str(seed)


##  watch -d -n 0.5 nvidia-smi

#from DLStudio import *

dls = DLStudio(
#                  dataroot = "/home/kak/TextDatasets/",
                  dataroot = "./Examples/data/",
                  path_saved_model = "./saved_model",
                  momentum = 0.9,
                  learning_rate =  0.005,  
                  epochs = 1,
                  batch_size = 1,
                  classes = ('negative','positive'),
                  debug_train = 1,
                  debug_test = 1,
                  use_gpu = True,
              )

import copy
def run_code_for_training_for_text_classification_no_gru(net, hidden_size, text_cl):        
           #filename_for_out = "performance_numbers_" + str(dls.epochs) + ".txt"
            #FILE = open(filename_for_out, 'w')
            net = copy.deepcopy(net)
            net = net.to(dls.device)
            criterion = nn.NLLLoss()
#            criterion = nn.MSELoss()
#            criterion = nn.CrossEntropyLoss()
            accum_times = []
            optimizer = optim.SGD(net.parameters(), 
                         lr=dls.learning_rate, momentum=dls.momentum)
            start_time = time.clock()
            for epoch in range(dls.epochs):  
                #print("")
                running_loss = 0.0
                for i, data in enumerate(text_cl.train_dataloader):    
                    hidden = torch.zeros(1, hidden_size)
                    hidden = hidden.to(dls.device)
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    review_tensor = review_tensor.to(dls.device)
                    sentiment = sentiment.to(dls.device)
                    optimizer.zero_grad()
                    input = torch.zeros(1,review_tensor.shape[2])
                    input = input.to(dls.device)
                    for k in range(review_tensor.shape[1]):
                        input[0,:] = review_tensor[0,k]
                        output, hidden = net(input, hidden)
                    loss = criterion(output, torch.argmax(sentiment,1))
                    running_loss += loss.item()
                    loss.backward(retain_graph=True)        
                    optimizer.step()
                    if i % 100 == 99:    
                        avg_loss = running_loss / float(100)
                        current_time = time.clock()
                        time_elapsed = current_time-start_time
                        print("[epoch:%d  iter:%4d]     loss: %.3f" % (epoch+1,i+1,avg_loss))
                        accum_times.append(current_time-start_time)
                        running_loss = 0.0
            
        
def run_code_for_testing_text_classification_no_gru(net, hidden_size, text_cl):
             #net.load_state_dict(torch.load(dls.path_saved_model))
            net = net.to(dls.device)
            classification_accuracy = 0.0
            negative_total = 0
            positive_total = 0
            confusion_matrix = torch.zeros(2,2)
            with torch.no_grad():
                for i, data in enumerate(text_cl.test_dataloader):
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    input = torch.zeros(1,review_tensor.shape[2]).to(dls.device)
                    hidden = torch.zeros(1, hidden_size)  
                    hidden=hidden.to(dls.device)       
                    for k in range(review_tensor.shape[1]):
                        input[0,:] = review_tensor[0,k]
                        output, hidden = net(input, hidden)
                    predicted_idx = torch.argmax(output).item()
                    gt_idx = torch.argmax(sentiment).item()
                    #if i % 100 == 99:
                    #    print("   [i=%4d]    predicted_label=%d       gt_label=%d" % (i+1, predicted_idx,gt_idx))
                    if predicted_idx == gt_idx:
                        classification_accuracy += 1
                    if gt_idx is 0: 
                        negative_total += 1
                    elif gt_idx is 1:
                        positive_total += 1
                    confusion_matrix[gt_idx,predicted_idx] += 1
            out_percent = np.zeros((2,2), dtype='float')
            #print("\n\nNumber of positive reviews tested: %d" % positive_total)
            #print("\n\nNumber of negative reviews tested: %d" % negative_total)
            print("\n\nconfusion matrix result:\n")
            out_str = "                      "
            out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
            print(out_str + "\n")
            for i,label in enumerate(['true negative', 'true positive']):
                out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
                out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
                out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
                out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
                out_str = "%12s:  " % label
                for j in range(2):
                    out_str +=  "%18s" % out_percent[i,j]
                print(out_str)

#!/usr/bin/env python

##  text_classification_no_gru.py

"""
This script shows how you can use a neural network with feedback for
the classification of a variable-length sequence.  The main idea is to
represent a variable-length input with a fixed-length hidden state 
vector.  

The reason for their being "_no_gru" in the name of the script is that
it does not use any gating mechanisms as a protection against the 
vanishing gradients.
"""



text_cl = DLStudio.TextClassification( dl_studio = dls )
dataserver_train = DLStudio.TextClassification.SentimentAnalysisDataset(
                                 train_or_test = 'train',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_train_3.tar.gz",
#                                 dataset_file = "sentiment_dataset_train_200.tar.gz",
                                dataset_file = "sentiment_dataset_train_40.tar.gz",
                                                                      )
dataserver_test = DLStudio.TextClassification.SentimentAnalysisDataset(
                                 train_or_test = 'test',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_test_3.tar.gz",
#                                 dataset_file = "sentiment_dataset_test_200.tar.gz",
                                 dataset_file = "sentiment_dataset_test_40.tar.gz",
                                                                  )
text_cl.dataserver_train = dataserver_train
text_cl.dataserver_test = dataserver_test

text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)

vocab_size = dataserver_train.get_vocab_size()
hidden_size = 512
output_size = 2                            # for positive and negative sentiments


##  DO NOT UNCOMMENT THE NEXT LINE UNLESS YOU KNOW WHAT YOU ARE DOING.
model = ModifiedTEXTnetOrder2(vocab_size, hidden_size, output_size, dls)

number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
num_layers = len(list(model.parameters()))

print("\n\nTask 2:")
run_code_for_training_for_text_classification_no_gru(model, hidden_size, text_cl)
run_code_for_testing_text_classification_no_gru(model, hidden_size, text_cl)

#Task 3

import random
import numpy
import torch
import os, sys


seed = 0           
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
numpy.random.seed(seed)
torch.backends.cudnn.deterministic=True
torch.backends.cudnn.benchmarks=False
os.environ['PYTHONHASHSEED'] = str(seed)


##  watch -d -n 0.5 nvidia-smi

#from DLStudio import *

dls = DLStudio(
#                  dataroot = "/home/kak/TextDatasets/",
                  dataroot = "./Examples/data/",
                  path_saved_model = "./saved_model",
                  momentum = 0.5,
                  learning_rate =  1e-4,
                  epochs = 1,
                  batch_size = 4,
                  classes = ('negative','positive'),
                  debug_train = 1,
                  debug_test = 1,
                  use_gpu = True,
              )


class LenEqualSentimentAnalysisDataset(DLStudio.TextClassification.SentimentAnalysisDataset, torch.utils.data.Dataset):
            def __init__(self, dl_studio, train_or_test, dataset_file, train_vocab_size =  None):
                super().__init__(dl_studio, train_or_test, dataset_file)
                self.train_vocab_size=train_vocab_size
               
            def one_hotvec_for_word(self, word):
                word_index =  self.vocab.index(word)
                hotv=torch.tensor(word_index)
                return hotv
            
            def get_max_r_length(self):
              if self.train_or_test == 'train':
                data = self.indexed_dataset_train 
                max_len=0
                for i in data:   
                    if max_len<len(i[0]):
                      max_len=len(i[0]) 
              else:
                max_len = self.train_vocab_size
              return max_len

            def review_to_tensor(self, review):
                max_l=  self.get_max_r_length()      
                review_tensor = torch.zeros(max_l)                
                for i,w in enumerate(review):
                    review_tensor[i] = self.one_hotvec_for_word(w)
                return review_tensor

def run_code_for_training_for_text_classification_with_gru(net, hidden_size, text_cl):        
            #filename_for_out = "performance_numbers_" + str(dls.epochs) + ".txt"
            #FILE = open(filename_for_out, 'w')
            net = copy.deepcopy(net)
            net = net.to(dls.device)
            criterion = nn.NLLLoss()
#            criterion = nn.MSELoss()
#            criterion = nn.CrossEntropyLoss()
            accum_times = []
            optimizer = optim.SGD(net.parameters(), 
                         lr=dls.learning_rate, momentum=dls.momentum)
            for epoch in range(dls.epochs):  
                #print("")
                running_loss = 0.0
                start_time = time.clock()
                for i, data in enumerate(text_cl.train_dataloader):    
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    review_tensor = review_tensor.to(dls.device)
                    sentiment = sentiment.to(dls.device)
                    optimizer.zero_grad()
                    hidden = net.init_hidden(review_tensor.shape[0]).to(dls.device)
                    #for k in range(review_tensor.shape[1]):
                    output, hidden = net(torch.unsqueeze(review_tensor,1), hidden)
                    loss = criterion(output, torch.argmax(sentiment, 1))
                    running_loss += loss.item()
                    loss.backward()
                    optimizer.step()
                    if i % 100 == 99:    
                        avg_loss = running_loss / float(100)
                        current_time = time.clock()
                        time_elapsed = current_time-start_time
                        print("[epoch:%d  iter:%4d]     loss: %.3f" % (epoch+1,i+1,avg_loss))
                        accum_times.append(current_time-start_time)
                        running_loss = 0.0

def run_code_for_testing_text_classification_with_gru(net, hidden_size, text_cl):
            #net.load_state_dict(torch.load(dls.path_saved_model))
            classification_accuracy = 0.0
            negative_total = 0
            positive_total = 0
            confusion_matrix = torch.zeros(2,2)
            net = net.to(dls.device)
            with torch.no_grad():
                for i, data in enumerate(text_cl.test_dataloader):
                    review_tensor,category,sentiment = data['review'], data['category'], data['sentiment']
                    review_tensor = review_tensor.to(dls.device)
                    sentiment = sentiment.to(dls.device)
                    #print(review_tensor.shape)
                    #return 0
                    hidden = net.init_hidden(review_tensor.shape[0]).to(dls.device)
                    #for k in range(review_tensor.shape[1]):
                    output, hidden = net(torch.unsqueeze(review_tensor,1), hidden)
                    for j in range(len(output)):
                      predicted_idx = torch.argmax(output[j]).item()
                      gt_idx = torch.argmax(sentiment[j]).item()
                      #if i % 100 == 99:
                      #    print("   [i=%d]    predicted_label=%d       gt_label=%d\n\n" % (i+1, predicted_idx,gt_idx))
                      if predicted_idx == gt_idx:
                          classification_accuracy += 1
                      if gt_idx is 0: 
                          negative_total += 1
                      elif gt_idx is 1:
                          positive_total += 1
                      confusion_matrix[gt_idx,predicted_idx] += 1
            out_percent = np.zeros((2,2), dtype='float')
            #print("\n\nNumber of positive reviews tested: %d" % positive_total)
            #print("\n\nNumber of negative reviews tested: %d" % negative_total)
            #print("\n\nDisplaying the confusion matrix:\n")
            out_str = "                      "
            out_str +=  "%18s    %18s" % ('predicted negative', 'predicted positive')
            print(out_str + "\n")
            for i,label in enumerate(['true negative', 'true positive']):
                out_percent[0,0] = "%.3f" % (100 * confusion_matrix[0,0] / float(negative_total))
                out_percent[0,1] = "%.3f" % (100 * confusion_matrix[0,1] / float(negative_total))
                out_percent[1,0] = "%.3f" % (100 * confusion_matrix[1,0] / float(positive_total))
                out_percent[1,1] = "%.3f" % (100 * confusion_matrix[1,1] / float(positive_total))
                out_str = "%12s:  " % label
                for j in range(2):
                    out_str +=  "%18s" % out_percent[i,j]
                print(out_str)

#!/usr/bin/env python

##  text_classification_with_gru.py

"""
This script is an attempt at solving the sentiment classification problem
with an RNN that uses a GRU to get around the problem of vanishing gradients
that are common to neural networks with feedback.
"""

text_cl = DLStudio.TextClassification( dl_studio = dls )
dataserver_train = LenEqualSentimentAnalysisDataset(
                                 train_or_test = 'train',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_train_3.tar.gz",
#                                dataset_file = "sentiment_dataset_train_200.tar.gz", 
                                 dataset_file = "sentiment_dataset_train_40.tar.gz", 
                                                                      )

text_cl.dataserver_train = dataserver_train

dataserver_test = LenEqualSentimentAnalysisDataset(
                                 train_or_test = 'test',
                                 dl_studio = dls,
#                                dataset_file = "sentiment_dataset_test_3.tar.gz",
#                                dataset_file = "sentiment_dataset_test_200.tar.gz",
                                 dataset_file = "sentiment_dataset_test_40.tar.gz", 
                                 train_vocab_size=dataserver_train.get_max_r_length()
                                                                  )

text_cl.dataserver_test = dataserver_test

text_cl.load_SentimentAnalysisDataset(dataserver_train, dataserver_test)


vocab_size = dataserver_train.get_max_r_length()
Evocab_size=1
hidden_size = 512
output_size = 2                            # for positive and negative sentiments
n_layers = 2

model = text_cl.GRUnet(vocab_size, hidden_size, output_size, n_layers)

number_of_learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

num_layers = len(list(model.parameters()))

print("\n\nTask 3:")
run_code_for_training_for_text_classification_with_gru(model, hidden_size, text_cl)
run_code_for_testing_text_classification_with_gru(model, hidden_size, text_cl)